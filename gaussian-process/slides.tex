\documentclass{beamer}
\usefonttheme[onlymath]{serif}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[export]{adjustbox}
\usepackage[utf8]{inputenc}
\usepackage{tikz} 
\usetikzlibrary{bayesnet}

\input{../command.tex}

%Information to be included in the title page:
\usecolortheme{seahorse}
\title{Gaussian Process}
\author{Kangcheng Hou}
\institute{Zhejiang University}
\date{\today}

    
    
\begin{document}
    
\frame{\titlepage}

\begin{frame}{Gaussian Process as function generator}
Gaussian Process is a distribution over function space, it can be used to represent uncertainty.
Samples from GP satisfies that any $d$ dimensional samples $(\mathbf{x}_1, \dots, \mathbf{x}_d)$ is from Gaussian distribution $\mathcal{N}(\mu(\mathbf{x}_1), \dots, \mu(\mathbf{x}_d)^\top, \mathbf{K}(\mathbf{x}_1, \dots, \mathbf{x}_d))$ 
The $\mu$ and $\mathbf{K}$ is chosen to represent our knowledge about the problem.
So provided some positions to be estimated $(\mathbf{x}_1, \dots, \mathbf{x}_d)$, GP can generate some functions and return the value estimated on those provided input points.
\end{frame}

\begin{frame}[allowframebreaks]{Inference in GP}
First we notice the properties of Gaussian distribuion,
$$p(\mathbf{f}, \mathbf{g}) = \mathcal{N}(\mat{\mathbf{a}\\\mathbf{b}}, \mat{A & C \\ C^\top &  B})$$
$$p(\mathbf{f} | \mathbf{g}) = \mathcal{N}(\mathbf{a} + CB^{-1}(\mathbf{y} - \mathbf{b}), A - C B^{-1} C^\top)$$
In this way, if we know the mean and the covariance of joint distribution of $(\mathbf{f}, \mathbf{g})$ and the observed $\mathbf{g}$. We can then infer the distribution of $\mathbf{f}$. 
What if we only saw a noisy observation, $\mathbf{y} \sim \mathcal{N}(\mathbf{g}, S)$?

\framebreak
$$p(\mathbf{f}, \mathbf{g}, \mathbf{y}) = p(\mathbf{f}, \mathbf{g}) p(\mathbf{y} | \mathbf{g})$$
is the product of two gaussian pdf, so joint distribution of $(\mathbf{f}, \mathbf{g}, \mathbf{y})$ is still Gaussian distributed.
Our posterior over $\mathbf{f}$ is still Gaussian:
$$p(\mathbf{f} | \mathbf{y}) \propto \int d \mathbf{g} p(\mathbf{f}, \mathbf{g}, \mathbf{y})$$
Thus, we can compute posterior over $\mathbf{f}$ given noisy observation $\mathbf{y}$.

\framebreak

Assuming that the data \textbf{are really sampled from } the GP we are using, given
\begin{align*}
\text{function }f \sim \mathcal{GP} \\ 
\mathbf{f} \sim \mathcal{N}(\mu, K) \\
y_i | f_i \sim \mathcal{N}(f_i, \sigma_n^2)
\end{align*}
We can do inference for test points $X_\ast$.
$$p(\mat{\mathbf{y} \\ \mathbf{f}_\ast}) = \mathcal{N}(\mathbf{0}, \mat{K(X, X) + \sigma_n^2 \mathbb{I} & K(X, X_\ast) \\ K(X_\ast, X) & K(X_\ast, X_\ast)})$$
Using the previous formula, we can calculate the conditional distritbuion $p(\mathbf{f}_\ast | \mathbf{y})$.
\end{frame}

\begin{frame}[allowframebreaks]{Hyper parameters}
Now if we specify the mean and covariance function, we can then do inference of the test points given the observed value $p(\mathbf{f}_\ast | \mathbf{y})$.
The natural question to ask is 
\begin{itemize}
\item How to determine the mean and covariance function?
\item What implication we are making if we specify the mean and covariance function?
\end{itemize}
Consider an example of Bayesian linear regression,
$$f(\mathbf{x}_i) = \mathbf{w}^\top \mathbf{x}_i + b, \quad \mathbf{w} \sim \mathcal{N}(0, \sigma_w^2 \mathbb{I}), b \sim \mathcal{N}(0, \sigma_b^2)$$
\begin{align*}
\text{cov}(f_i, f_j) & = \langle f_i, f_j \rangle - \langle f_i \rangle \langle f_j \rangle \\ 
& = \langle \mathbf{w}^\top \mathbf{x}_i + b, \mathbf{w}^\top \mathbf{x}_j + b \rangle \\
& = \sigma_w^2 \mathbf{x}_i^\top \mathbf{x}_j + \sigma_b^2 = k(\mathbf{x}_i, \mathbf{x}_j)
\end{align*}
This means the Bayesian linear regression is equivalent to the $k(\mathbf{x}_i, \mathbf{x}_j) = \sigma_w^2 \mathbf{x}_i^\top \mathbf{x}_j + \sigma_b^2$ kernel. 




\end{frame}



\end{document}